import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
from xgboost import XGBClassifier
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import joblib

# ðŸ”¹ Chargement du dataset
df = pd.read_csv('transactions.csv')

# ðŸ”¹ Suppression des doublons
df = df.drop_duplicates()

# ðŸ”¹ SÃ©paration features (X) et cible (y)
X = df.drop('is_fraud', axis=1)
y = df['is_fraud']

# ðŸ”¹ PrÃ©traitement des colonnes
if 'timestamp' in X.columns:
    X['timestamp'] = pd.to_datetime(X['timestamp'])
    X['transaction_hour'] = X['timestamp'].dt.hour
    X['transaction_day'] = X['timestamp'].dt.dayofweek
    X['transaction_month'] = X['timestamp'].dt.month
    X['transaction_week'] = X['timestamp'].dt.isocalendar().week
    X['timestamp'] = X['timestamp'].view('int64') // 10**9  # Conversion en secondes UNIX

# ðŸ”¹ Encodage des colonnes catÃ©goriques
categorical_cols = X.select_dtypes(include=['object']).columns
for col in categorical_cols:
    X[col] = X[col].astype('category').cat.codes

# ðŸ”¹ Conversion des types pour Ã©viter les erreurs avec SMOTE
X = X.astype(np.float64)

# ðŸ”¹ Normalisation des donnÃ©es numÃ©riques
numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns
scaler = StandardScaler()
X[numeric_cols] = scaler.fit_transform(X[numeric_cols])

# ðŸ”¹ Sauvegarde du StandardScaler pour `app.py`
joblib.dump(scaler, "scaler.pkl")

# ðŸ”¹ Division train/test avec `random_state`
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# ðŸ”¹ RÃ©Ã©chantillonnage avec SMOTE (Ã©viter trop de fraudes)
smote = SMOTE(sampling_strategy=0.3, random_state=42)  # Ajuster pour Ã©viter le surÃ©chantillonnage
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

# ðŸ”¹ VÃ©rification aprÃ¨s SMOTE
print("Distribution aprÃ¨s SMOTE:")
print(y_train_resampled.value_counts())

# ðŸ”¹ Initialisation du modÃ¨le XGBoost avec `random_state`
xgb = XGBClassifier(
    n_estimators=500,
    learning_rate=0.1,
    max_depth=4,
    min_child_weight=3,
    subsample=0.7,
    colsample_bytree=0.7,
    gamma=1,
    reg_alpha=2,
    reg_lambda=2,
    random_state=42
)

# ðŸ”¹ EntraÃ®nement du modÃ¨le
xgb.fit(X_train_resampled, y_train_resampled)

# ðŸ”¹ Sauvegarde du modÃ¨le entraÃ®nÃ©
joblib.dump(xgb, 'best_xgboost_model.h5')

# ðŸ”¹ VÃ©rification des features importantes
plt.figure(figsize=(10, 6))
plt.bar(range(len(xgb.feature_importances_)), xgb.feature_importances_)
plt.xlabel("Feature Index")
plt.ylabel("Importance")
plt.title("Importance des Features dans XGBoost")
plt.show()

# ðŸ”¹ Ã‰valuation du modÃ¨le
y_pred_xgb = xgb.predict(X_test)
print(classification_report(y_test, y_pred_xgb))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_xgb))
